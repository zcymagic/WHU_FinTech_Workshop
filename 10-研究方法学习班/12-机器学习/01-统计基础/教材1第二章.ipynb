{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二章 Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 What Is Statistical Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, statistical learning refers to a set of approaches for estimating f(x)： $Y=f(X)+\\epsilon$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Why Estimate f?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main reasons that we may wish to estimate for f: prediction and inference.\n",
    "\n",
    "Recommendation: 基本无害的计量经济学（Mostly Harmless Econometrics，Angrist and Pischke, 2009)；教材04-基本有用的计量经济学"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 How Do We Estimate f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parametric methods：** two-step model-based approach， First, we make an assumption about the functional form, or shape,\n",
    "of f. After a model has been selected, we need a procedure that uses the training data to fit or train the model. The most common approach is referred to as (ordinary) least squares.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/line.png)\n",
    "![title](img/line2.png)\n",
    "![title](img/img1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-parametric methods:** Non-parametric methods do not make explicit assumptions about the functional form of f. Instead they seek an estimate of f that gets as close to the data points as possible without being too rough or wiggly. \n",
    "\n",
    "Such approaches can have a major advantage over parametric approaches: by avoiding the assumption of a particular functional form for f, they have the potential to accurately fit a wider range of possible shapes for f."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 The Trade-off Between Prediction Accuracy and Model Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A representation of the tradeoff between flexibility and interpretability, using different statistical learning methods. In general, as the flexibility of a method increases, its interpretability decreases.\n",
    " \n",
    "**Recommendation:** Bao Y, Ke B, Li B, et al. Detecting accounting fraud in publicly traded US firms using a machine learning approach[J]. Journal of Accounting Research, 2020, 58(1): 199-235."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/img3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Supervised versus Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most statistical learning problems fall into one of two categories: supervised or unsupervised.\n",
    "\n",
    "Supervised :For each observation of the predictor measurement(s) xi, i = 1, . . . , n there is an associated response measurement yi.\n",
    "\n",
    "Unsupervised: describes the somewhat more challenging situation in which for every observation i = 1, . . . , n, we observe a vector of measurements xi but no associated response yi.\n",
    "\n",
    "Semi-supervised learning:. For m of the observations, where m < n, we have both predictor measurements and a response measurement. For the remaining n − m observations, we have predictor measurements but no response measurement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Regression versus Classification Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables can be characterized as either quantitative or qualitative (also known as categorical). Quantitative variables take on numerical values. Examples include a person's age, height, or income, the value of a house, and the price of a stock.   \n",
    "\n",
    "In contrast, qualitative variables take on values in one of different classes , or categories. Examples of qualitative variables include a person’s gender (male or female), the brand of product purchased (brand A, B, or C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## 2.2 Assessing Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Measuring the Quality of Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " mean squared error (MSE):also known as train MSE\n",
    "![title](img/line3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/img4.png)\n",
    "Left: Data simulated from f, shown in black. Three estimates of f are shown: the linear regression line (orange curve), and two smoothing spline fits (blue and green curves). \n",
    "\n",
    "Right: Training MSE (grey curve), test MSE (red curve), and minimum possible test MSE over all methods (dashed line). Squares represent the training and test MSEs for the three fits shown in the left-hand panel.\n",
    "\n",
    "**Hint：** 模型复杂度不是越高越好，复杂度越高，越容易发生过拟合情况，导致模型的泛化能力减弱。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 The Bias-Variance Trade-Of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected test MSE： ![title](img/line5.png)\n",
    "\n",
    "**Variance** refers to the amount by which $\\hat{f}$ would change if we\n",
    "estimated it using a different training data set.\n",
    "If a method has high variance then small changes in the training data can result in large changes in $\\hat{f}$.   In general,more flexible statistical methods have higher variance.\n",
    "\n",
    "**Bias** refers to the error that is introduced by approximating\n",
    "a real-life problem, which may be extremely complicated, by a much\n",
    "simpler model. \n",
    "\n",
    "![title](img/img5.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 The Classification Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training error rate :\n",
    "![title](img/line6.png)\n",
    "test error rate:\n",
    "![title](img/line7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes classifier:**  $\\operatorname{Pr}\\left(Y=j \\mid X=x_{0}\\right)$\n",
    "\n",
    "Note that this is a conditional probability: it is the probability conditional\n",
    "probability that Y = j, given the observed predictor vector $x_{0}$. \n",
    "\n",
    "Recommendation: https://zhuanlan.zhihu.com/p/149774236 (贝叶斯分类器)\n",
    "\n",
    "In a two-class problem where there are only two possible response values, say class 1 or class 2, the Bayes classifier corresponds to predicting class one if Pr(Y = 1|X = x0) > 0.5, and class two otherwise.\n",
    "![title](img/line10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-nearest neighbors (KNN) classifier**:\n",
    "Given a positive integer K and a test observation x0, the KNN classifier first identifies the\n",
    "K points in the training data that are closest to x0, represented by N0. It then estimates the conditional probability for class j as the fraction of\n",
    "points in N0 whose response values equal j:![title](img/line9.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
